{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST æ•°æ®é›†çš„ç›¸å…³å¸¸æ•°\n",
    "# è¾“å…¥å±‚çš„èŠ‚ç‚¹æ•°ã€‚å¯¹äºMNISTæ•°æ®é›†ï¼Œè¿™ä¸ªå°±ç­‰äºå›¾ç‰‡çš„åƒç´ \n",
    "INPUT_NODE = 784\n",
    "# è¾“å‡ºå±‚çš„èŠ‚ç‚¹æ•°ã€‚è¿™ä¸ªç­‰äºç±»åˆ«çš„æ•°ç›®ã€‚å› ä¸ºåœ¨MNISTæ•°æ®é›†ä¸­\n",
    "# éœ€è¦åŒºåˆ†çš„æ˜¯0~9è¿™10ä¸ªæ•°å­—ï¼Œæ‰€ä»¥è¿™é‡Œè¾“å‡ºå±‚çš„èŠ‚ç‚¹æ•°ä¸º10\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "# é…ç½®ç¥ç»ç½‘ç»œå‚æ•°\n",
    "# éšè—å±‚èŠ‚ç‚¹æ•°ã€‚è¿™é‡Œä½¿ç”¨åªæœ‰ä¸€ä¸ªéšè—å±‚çš„ç½‘ç»œç»“æ„ä½œä¸ºæ ·ä¾‹\n",
    "# è¿™ä¸ªéšè—å±‚æœ‰500ä¸ªèŠ‚ç‚¹\n",
    "LAYER1_NODE = 500\n",
    "\n",
    "# ä¸€ä¸ªè®­ç»ƒbatchä¸­çš„è®­ç»ƒæ•°æ®ä¸ªæ•°ã€‚æ•°å­—è¶Šå°æ—¶ï¼Œè®­ç»ƒè¿‡ç¨‹çº¦æ¥è¿‘éšæœºæ¢¯åº¦ä¸‹é™ï¼›\n",
    "# æ•°å­—è¶Šå¤§æ—¶ï¼Œè®­ç»ƒè¶Šæ¥è¿‘æ¢¯åº¦ä¸‹é™\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# åŸºç¡€å­¦ä¹ ç‡\n",
    "LEARNING_RATE_BASE = 0.8\n",
    "# å­¦ä¹ ç‡çš„è¡°å‡ç‡\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "# æè¿°æ¨¡å‹å¤æ‚åº¦çš„æ­£åˆ™åŒ–é¡¹åœ¨æŸå¤±å‡½æ•°ä¸­çš„ç³»æ•°\n",
    "REGULARAZTION_RATE = 0.0001\n",
    "# è®­ç»ƒè½®æ•°\n",
    "TRAINING_STEPS = 30000\n",
    "# æ»‘åŠ¨å¹³å‡è¡°å‡ç‡\n",
    "MOVING_AVERAGE_DECAY = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œç»™å®šç¥ç»ç½‘ç»œçš„è¾“å…¥å’Œæ‰€æœ‰å‚æ•°ï¼Œè®¡ç®—ç¥ç»ç½‘ç»œçš„å‰å‘ä¼ æ’­ç»“æœã€‚åœ¨è¿™é‡Œ\n",
    "# å®šä¹‰äº†ä¸€ä¸ªä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°çš„ä¸‰å±‚å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼Œé€šè¿‡åŠ å…¥éšè—å±‚æ—¶é—´äº†å¤šå±‚ç¥ç»ç½‘ç»œç»“æ„\n",
    "# é€šè¿‡ReLUæ¿€æ´»å‡½æ•°å®ç°äº†å»çº¿æ€§åŒ–ã€‚åœ¨è¿™ä¸ªå‡½æ•°ä¸­ä¹Ÿæ”¯æŒä¼ å…¥ç”¨äºè®¡ç®—å‚æ•°å¹³å‡å€¼çš„ç±»\n",
    "# è¿™æ ·æ–¹ä¾¿åœ¨æµ‹è¯•æ—¶ä½¿ç”¨æ»‘åŠ¨å¹³å‡æ¨¡å‹\n",
    "def inference(input_tensor, avg_class, weights1, biases1, weights2, biases2):\n",
    "    # å½“æ²¡æœ‰æä¾›æ¬¢åŠ¨å¹³å‡ç±»æ—¶ï¼Œç›´æ¥ä½¿ç”¨å‚æ•°å½“å‰çš„å–å€¼\n",
    "    if avg_class is None:\n",
    "        # è®¡ç®—éšè—å±‚çš„å‰å‘ä¼ æ’­ç»“æœï¼Œè¿™é‡Œä½¿ç”¨äº†ReLUæ¿€æ´»å‡½æ•°\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)\n",
    "        # è®¡ç®—è¾“å‡ºå±‚çš„å‰å‘ä¼ æ’­ç»“æœã€‚å› ä¸ºåœ¨è®¡ç®—æŸå¤±å‡½æ•°æ—¶ä¼šä¸€å¹¶è®¡ç®— sofmaxå‡½æ•°ï¼Œ\n",
    "        # æ‰€ä»¥è¿™é‡Œä¸éœ€è¦åŠ å…¥æ¿€æ´»å‡½æ•°ã€‚è€Œä¸”ä¸åŠ å…¥softmaxä¸ä¼šå½±å“é¢„æµ‹ç»“æœã€‚å› ä¸ºé¢„æµ‹æ—¶\n",
    "        # ä½¿ç”¨çš„æ˜¯ä¸åŒç±»åˆ«çš„èŠ‚ç‚¹è¾“å‡ºå€¼çš„ç›¸å¯¹å¤§å°ï¼Œæœ‰æ²¡æœ‰softmaxå±‚å¯¹æœ€åçš„åˆ†ç±»ç»“æœçš„\n",
    "        # è®¡ç®—æ²¡æœ‰å½±å“ã€‚é¢„ç®—åœ¨è®¡ç®—æ•´ä¸ªç¥ç»ç½‘ç»œçš„å‰å‘ä¼ æ’­æ—¶å¯ä»¥ä¸åŠ å…¥æœ€åçš„softmaxå±‚ã€‚\n",
    "        return tf.matmul(layer1, weights2) + biases2\n",
    "    else:\n",
    "        # é¦–å…ˆä½¿ç”¨avg_class.averageå‡½æ•°æ¥è®¡ç®—å¾—å‡ºå˜é‡çš„æ»‘åŠ¨å¹³å‡å€¼\n",
    "        # ç„¶åå†è®¡ç®—ç›¸åº”çš„ç¥ç»ç½‘ç»œå‰å‘ä¼ æ’­ç»“æœ\n",
    "        # exp4.0\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1))\n",
    "        return tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mnist):\n",
    "    x = tf.placeholder(tf.float32, [None, INPUT_NODE], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name='y-input')\n",
    "    # ç”Ÿæˆéšè—å±‚å‚æ•°\n",
    "    weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))\n",
    "    biases1 = tf.Variable(tf.constant(0.1, shape=[LAYER1_NODE]))\n",
    "\n",
    "    # ç”Ÿæˆè¾“å‡ºå±‚å‚æ•°\n",
    "    weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=0.1))\n",
    "    biases2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))\n",
    "\n",
    "    # è®¡ç®—ä¸å«æ»‘åŠ¨å¹³å‡ç±»çš„å‰å‘ä¼ æ’­ç»“æœã€‚è¿™é‡Œç»™å‡ºçš„ç”¨äºè®¡ç®—æ»‘åŠ¨å¹³å‡çš„ç±»ä¸ºNoneï¼Œ\n",
    "    # æ‰€ä»¥å‡½æ•°ä¸ä¼šä½¿ç”¨å‚æ•°çš„æ»‘åŠ¨å¹³å‡å€¼\n",
    "    y = inference(x, None, weights1, biases1, weights2, biases2)\n",
    "\n",
    "    # å®šä¹‰è®­ç»ƒè½®æ•°åŠç›¸å…³æ»‘åŠ¨å¹³å‡ç±»\n",
    "    # å®šä¹‰å­˜å‚¨è®­ç»ƒè½®æ•°çš„å˜é‡ã€‚è¿™ä¸ªå˜é‡ä¸éœ€è¦è®¡ç®—æ»‘åŠ¨å¹³å‡å€¼ï¼Œæ‰€ä»¥è¿™é‡Œåˆ¶å®šè¿™ä¸ªå˜é‡ä¸º\n",
    "    # ä¸å¯è®­ç»ƒçš„å˜é‡ï¼ˆtrainable=Falseï¼‰ã€‚åœ¨ä½¿ç”¨TensorFlowè®­ç»ƒç¥ç»ç½‘ç»œæ—¶ï¼Œ\n",
    "    # ä¸€èˆ¬ä¼šå°†ä»£è¡¨è®­ç»ƒè½®æ•°çš„å˜é‡åˆ¶å®šä¸ºä¸å¯è®­ç»ƒçš„å‚æ•°\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    # ç»™å®šæ»‘åŠ¨å¹³å‡è¡°å‡ç‡å’Œè®­ç»ƒè½®æ•°çš„å˜é‡ï¼Œåˆå§‹åŒ–æ»‘åŠ¨å¹³å‡ç±»ã€‚\n",
    "    # ç»™å®šè®­ç»ƒè½®æ•°çš„å˜é‡å¯ä»¥åŠ å¿«è®­ç»ƒæ—©èµ·å˜é‡çš„æ›´æ–°é€Ÿåº¦\n",
    "    # exp1.0\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "\n",
    "    # åœ¨æ‰€æœ‰ä»£è¡¨ç¥ç»ç½‘ç»œå‚æ•°çš„å˜é‡ä¸Šä½¿ç”¨æ»‘åŠ¨å¹³å‡ã€‚å…¶ä»–è¾…åŠ©å˜é‡ï¼ˆæ¯”å¦‚global_stepï¼‰å°±\n",
    "    # ä¸éœ€è¦äº†ã€‚tf.trainable_variablesè¿”å›çš„å°±æ˜¯å›¾ä¸Šçš„é›†åˆ\n",
    "    # GraphKeys.TRAINABLE_VARIABLESä¸­çš„å…ƒç´ ã€‚è¿™ä¸ªé›†åˆçš„å…ƒç´ å°±æ˜¯æ‰€æœ‰æ²¡æœ‰æŒ‡å®š\n",
    "    # trainable=False çš„å‚æ•°\n",
    "    # exp2.0\n",
    "    variable_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "    # è®¡ç®—ä½¿ç”¨äº†æ»‘åŠ¨å¹³å‡ä¹‹åçš„å‘å‰ä¼ æ’­ç»“æœã€‚\n",
    "    # æ»‘åŠ¨å¹³å‡ä¸ä¼šæ”¹å˜æœ¬äº‹çš„å–å€¼\n",
    "    # è€Œæ˜¯ç»´æŠ¤äº†ä¸€ä¸ªå½±å­å˜é‡æ¥è®°å½•æ»‘åŠ¨å¹³å‡å€¼ã€‚æ‰€ä»¥å½“éœ€è¦ä½¿ç”¨è¿™ä¸ªæ»‘åŠ¨å¹³å‡å€¼æ—¶ï¼Œéœ€è¦æ˜ç¡®è°ƒç”¨averageå‡½æ•°\n",
    "    average_y = inference(x, variable_averages, weights1, biases1, weights2, biases2)\n",
    "\n",
    "    # è®¡ç®—äº¤å‰ç†µæœºå™¨å¹³å‡å€¼\n",
    "    # è®¡ç®—äº¤å‰ç†µä½œä¸ºåˆ»ç”»é¢„æµ‹å€¼å’ŒçœŸå®å€¼ä¹‹é—´å·®è·çš„æŸå¤±å‡½æ•°ã€‚è¿™é‡Œä½¿ç”¨äº†TensorFlowä¸­æä¾›çš„\n",
    "    # sparse_softmax_cross_entropy_with_logitså‡½æ•°æ¥è®¡ç®—äº¤å‰ç†µã€‚å½“åˆ†ç±»\n",
    "    # é—®é¢˜åªæœ‰ä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆæ—¶ï¼Œå¯ä»¥ä½¿ç”¨è¿™ä¸ªå‡½æ•°æ¥åŠ é€Ÿäº¤å‰ç†µçš„è®¡ç®—ã€‚MNISTé—®é¢˜çš„å›¾ç‰‡ä¸­\n",
    "    # å‚æ•°æ˜¯ç¥ç»ç½‘ç»œä¸åŒ…æ‹¬softmaxå±‚çš„å‰å‘ä¼ æ’­ç»“æœï¼Œç¬¬äºŒä¸ªæ˜¯è®­ç»ƒæ•°æ®çš„æ­£ç¡®ç­”æ¡ˆã€‚å› ä¸ºæ ‡å‡†å¤§éš¾æ˜¯é•¿åº¦ä¸º10çš„ä¸€ç»´æ•°ç»„\n",
    "    # è€Œè¯¥å‡½æ•°éœ€è¦æä¾›çš„æ˜¯æ­£ç¡®ç­”æ¡ˆğŸ”¢ï¼Œæ‰€ä»¥éœ€è¦ä½¿ç”¨tf.arg_maxæ¥å¾—åˆ°æ­£ç¡®ç­”æ¡ˆå¯¹äºçš„ç±»åˆ«ç¼–å·\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.arg_max(y_, 1))\n",
    "    # è®¡ç®—åœ¨å½“å‰batchä¸­æ‰€æœ‰æ ·ä¾‹çš„äº¤å‰ç†µå¹³å‡å€¼\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    # è®¡ç®—L2æ­£åˆ™åŒ–æŸå¤±å‡½æ•°\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\n",
    "    # è®¡ç®—æ¨¡å‹çš„æ­£åˆ™åŒ–æŸå¤±ã€‚ä¸€èˆ¬å­è®¡ç®—ç¥ç»ç½‘ç»œè¾¹ä¸Šæƒé‡çš„æ­£åˆ™åŒ–æŸå¤±ï¼Œè€Œä¸ä½¿ç”¨åç½®é¡¹ã€‚\n",
    "    regularaztion = regularizer(weights1) + regularizer(weights2)\n",
    "    # æ€»æŸå¤±ç­‰äºäº¤å‰ç†µæŸå¤±å’Œæ­£æŠ“æŸå¤±çš„å’Œ\n",
    "    loss = cross_entropy_mean + regularaztion\n",
    "\n",
    "    # è®¾ç½®æŒ‡æ•°è¡°å‡çš„å­¦ä¹ ç‡\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,  # åŸºç¡€çš„å­¦ä¹ ç‡ï¼Œéšç€è¿­ä»£çš„è¿›è¡Œï¼Œæ›´æ–°å˜é‡æ—¶ä½¿ç”¨å­¦ä¹ ç‡åœ¨è¿™ä¸ªåŸºç¡€ä¸Šé€’å‡\n",
    "        global_step,  # å½“å‰è¿­ä»£çš„è½®æ•°\n",
    "        mnist.train.num_examples / BATCH_SIZE,  # è¿‡å®Œæ‰€æœ‰çš„è®­ç»ƒæ•°æ®éœ€è¦çš„è¿­ä»£æ¬¡æ•°\n",
    "        LEARNING_RATE_DECAY,  # å­¦ä¹ ç‡è¡°å‡é€Ÿåº¦\n",
    "        staircase=True\n",
    "    )\n",
    "\n",
    "    # ä¼˜åŒ–æŸå¤±å‡½æ•°\n",
    "    # ä½¿ç”¨ tf.train.GradientDescentOptimizer ä¼˜åŒ–ç®—æ³•æ¥ä¼˜åŒ–æŸå¤±å‡½æ•°ã€‚æ³¨æ„è¿™é‡ŒæŸå¤±å‡½æ•°\n",
    "    # åŒ…å«äº†äº¤å‰ç†µæŸå¤±å’ŒL2æ­£åˆ™åŒ–æŸå¤±\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # éœ€è¦train_step, variable_averages_opæ‰§è¡Œå®Œæˆåï¼Œæ‰èƒ½å¾€ä¸‹æ‰§è¡Œã€‚\n",
    "    # åå‘ä¼ æ’­æ›´æ–°å‚æ•°å’Œæ›´æ–°æ¯ä¸€ä¸ªå‚æ•°çš„æ»‘åŠ¨å¹³å‡å€¼\n",
    "    # train_op = tf.group(train_step, variable_averages_op) ç­‰åŒå¦‚ä¸‹ä¸‹ä¸¤è¡Œ\n",
    "    # exp3.0\n",
    "    with tf.control_dependencies([train_step, variable_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "\n",
    "    # è®¡ç®—æ­£ç¡®ç‡\n",
    "    correct_prediction = tf.equal(tf.argmax(average_y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # åˆå§‹åŒ–ä¼šè¯å¹¶å¼€å§‹è®­ç»ƒè¿‡ç¨‹\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        validate_feed = {x: mnist.validation.images, y_: mnist.validation.labels}\n",
    "        test_feed = {x: mnist.test.images, y_: mnist.test.labels}\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            if i % 1000 == 0:\n",
    "                validate_acc = sess.run(accuracy, feed_dict=validate_feed)\n",
    "                test_acc = sess.run(accuracy, feed_dict=test_feed)\n",
    "                print(\"After %d training step(s), validation accuracy using average model is %g,\"\n",
    "                      \"test accuracy using average model is %g \" % (i, validate_acc, test_acc))\n",
    "\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(train_op, feed_dict={x: xs, y_: ys})\n",
    "\n",
    "        # è®¡ç®—æ»‘åŠ¨å¹³å‡æ¨¡å‹å’‹æµ‹è¯•æ•°æ®å’ŒéªŒè¯æ•°æ®ä¸Šçš„æ­£ç¡®ç‡\n",
    "        test_acc = sess.run(accuracy, feed_dict=test_feed)\n",
    "        print((\"After %d training step(s), test accuracy using average model is %g\" % (TRAINING_STEPS, test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets(\"../datasets/MNIST_data\", one_hot=True)\n",
    "    train(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-f6d73c1aa5c2>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../datasets/MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../datasets/MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../datasets/MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ../datasets/MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-4-4b9e09b38f0c>:42: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n",
      "After 0 training step(s), validation accuracy using average model is 0.1248,test accuracy using average model is 0.1358 \n",
      "After 1000 training step(s), validation accuracy using average model is 0.9782,test accuracy using average model is 0.9766 \n",
      "After 2000 training step(s), validation accuracy using average model is 0.9814,test accuracy using average model is 0.9819 \n",
      "After 3000 training step(s), validation accuracy using average model is 0.9828,test accuracy using average model is 0.9828 \n",
      "After 4000 training step(s), validation accuracy using average model is 0.9844,test accuracy using average model is 0.9833 \n",
      "After 5000 training step(s), validation accuracy using average model is 0.984,test accuracy using average model is 0.984 \n",
      "After 6000 training step(s), validation accuracy using average model is 0.9848,test accuracy using average model is 0.9836 \n",
      "After 7000 training step(s), validation accuracy using average model is 0.9838,test accuracy using average model is 0.9843 \n",
      "After 8000 training step(s), validation accuracy using average model is 0.9846,test accuracy using average model is 0.9835 \n",
      "After 9000 training step(s), validation accuracy using average model is 0.9842,test accuracy using average model is 0.9843 \n",
      "After 10000 training step(s), validation accuracy using average model is 0.984,test accuracy using average model is 0.9846 \n",
      "After 11000 training step(s), validation accuracy using average model is 0.984,test accuracy using average model is 0.9846 \n",
      "After 12000 training step(s), validation accuracy using average model is 0.9848,test accuracy using average model is 0.9845 \n",
      "After 13000 training step(s), validation accuracy using average model is 0.9852,test accuracy using average model is 0.9845 \n",
      "After 14000 training step(s), validation accuracy using average model is 0.9848,test accuracy using average model is 0.9842 \n",
      "After 15000 training step(s), validation accuracy using average model is 0.9852,test accuracy using average model is 0.9841 \n",
      "After 16000 training step(s), validation accuracy using average model is 0.9858,test accuracy using average model is 0.9842 \n",
      "After 17000 training step(s), validation accuracy using average model is 0.9852,test accuracy using average model is 0.9845 \n",
      "After 18000 training step(s), validation accuracy using average model is 0.9864,test accuracy using average model is 0.9847 \n",
      "After 19000 training step(s), validation accuracy using average model is 0.986,test accuracy using average model is 0.9842 \n",
      "After 20000 training step(s), validation accuracy using average model is 0.986,test accuracy using average model is 0.9842 \n",
      "After 21000 training step(s), validation accuracy using average model is 0.9866,test accuracy using average model is 0.984 \n",
      "After 22000 training step(s), validation accuracy using average model is 0.9862,test accuracy using average model is 0.9841 \n",
      "After 23000 training step(s), validation accuracy using average model is 0.986,test accuracy using average model is 0.9845 \n",
      "After 24000 training step(s), validation accuracy using average model is 0.9862,test accuracy using average model is 0.9845 \n",
      "After 25000 training step(s), validation accuracy using average model is 0.9864,test accuracy using average model is 0.9847 \n",
      "After 26000 training step(s), validation accuracy using average model is 0.9862,test accuracy using average model is 0.9845 \n",
      "After 27000 training step(s), validation accuracy using average model is 0.9866,test accuracy using average model is 0.9844 \n",
      "After 28000 training step(s), validation accuracy using average model is 0.9862,test accuracy using average model is 0.9846 \n",
      "After 29000 training step(s), validation accuracy using average model is 0.986,test accuracy using average model is 0.9845 \n",
      "After 30000 training step(s), test accuracy using average model is 0.9845\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
